{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-Bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNngPKkOJ48ZSdMmR7nm9kM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibcasxxf/root/blob/master/test_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVUpRanLYHRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6fab9c-0301-4580-b8fb-f830af650f92"
      },
      "source": [
        "pip install -r /home/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 2)) (3.3.1)\n",
            "Requirement already satisfied: joblib==0.16.0 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: tqdm==4.47.0 in /usr/local/lib/python3.7/dist-packages (from -r /home/requirements.txt (line 6)) (4.47.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0->-r /home/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (21.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /home/requirements.txt (line 2)) (0.8.1rc2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r /home/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->-r /home/requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /home/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /home/requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /home/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /home/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /home/requirements.txt (line 2)) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtUrJBVN61FQ"
      },
      "source": [
        "%load /content/src/model.py\n",
        "%load /content/src/train.py\n",
        "%load /content/src/trainer.py\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfJSEvm7Y_F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999035f1-f4dc-469e-ecdf-c0d95b39557a"
      },
      "source": [
        "%run /content/src/train.py -h"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] [--dataset_dir DATASET_DIR]\n",
            "                [--label_names_file LABEL_NAMES_FILE]\n",
            "                [--train_file TRAIN_FILE] [--test_file TEST_FILE]\n",
            "                [--test_label_file TEST_LABEL_FILE]\n",
            "                [--final_model FINAL_MODEL] [--out_file OUT_FILE]\n",
            "                [--eval_batch_size EVAL_BATCH_SIZE]\n",
            "                [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                [--top_pred_num TOP_PRED_NUM]\n",
            "                [--category_vocab_size CATEGORY_VOCAB_SIZE]\n",
            "                [--match_threshold MATCH_THRESHOLD] [--max_len MAX_LEN]\n",
            "                [--update_interval UPDATE_INTERVAL]\n",
            "                [--accum_steps ACCUM_STEPS] [--mcp_epochs MCP_EPOCHS]\n",
            "                [--self_train_epochs SELF_TRAIN_EPOCHS] [--early_stop]\n",
            "                [--gpus GPUS] [--dist_port DIST_PORT]\n",
            "\n",
            "main\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataset_dir DATASET_DIR\n",
            "                        dataset directory (default: datasets/agnews/)\n",
            "  --label_names_file LABEL_NAMES_FILE\n",
            "                        file containing label names (under dataset directory)\n",
            "                        (default: label_names.txt)\n",
            "  --train_file TRAIN_FILE\n",
            "                        unlabeled text corpus for training (under dataset\n",
            "                        directory); one document per line (default: train.txt)\n",
            "  --test_file TEST_FILE\n",
            "                        test corpus to conduct model predictions (under\n",
            "                        dataset directory); one document per line (default:\n",
            "                        None)\n",
            "  --test_label_file TEST_LABEL_FILE\n",
            "                        test corpus ground truth label; if provided, model\n",
            "                        will be evaluated during self-training (default: None)\n",
            "  --final_model FINAL_MODEL\n",
            "                        the name of the final classification model to save to\n",
            "                        (default: final_model.pt)\n",
            "  --out_file OUT_FILE   model predictions on the test corpus if provided\n",
            "                        (default: out.txt)\n",
            "  --eval_batch_size EVAL_BATCH_SIZE\n",
            "                        batch size per GPU for evaluation; bigger batch size\n",
            "                        makes training faster (default: 128)\n",
            "  --train_batch_size TRAIN_BATCH_SIZE\n",
            "                        batch size per GPU for training (default: 32)\n",
            "  --top_pred_num TOP_PRED_NUM\n",
            "                        language model MLM top prediction cutoff (default: 50)\n",
            "  --category_vocab_size CATEGORY_VOCAB_SIZE\n",
            "                        category vocabulary size for each class (default: 100)\n",
            "  --match_threshold MATCH_THRESHOLD\n",
            "                        category indicative words matching threshold (default:\n",
            "                        20)\n",
            "  --max_len MAX_LEN     length that documents are padded/truncated to\n",
            "                        (default: 512)\n",
            "  --update_interval UPDATE_INTERVAL\n",
            "                        self training update interval; 50 is good in general\n",
            "                        (default: 50)\n",
            "  --accum_steps ACCUM_STEPS\n",
            "                        gradient accumulation steps during training (default:\n",
            "                        1)\n",
            "  --mcp_epochs MCP_EPOCHS\n",
            "                        masked category prediction training epochs; 3-5\n",
            "                        usually is good depending on dataset size (smaller\n",
            "                        dataset needs more epochs) (default: 3)\n",
            "  --self_train_epochs SELF_TRAIN_EPOCHS\n",
            "                        self training epochs; 1-5 usually is good depending on\n",
            "                        dataset size (smaller dataset needs more epochs)\n",
            "                        (default: 1)\n",
            "  --early_stop          whether or not to enable early stop of self-training\n",
            "                        (default: False)\n",
            "  --gpus GPUS           number of gpus to use (default: 3)\n",
            "  --dist_port DIST_PORT\n",
            "                        distributed training port id; any number between 10000\n",
            "                        and 20000 will work (default: 12345)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxniAj-ryCCP",
        "outputId": "b0b1b3ba-8910-4f58-b488-ae28ecf29dee"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov  5 04:18:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    37W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u73egU8Msuwh",
        "outputId": "d1fee6ec-e6a4-4f61-da46-008ccc1b3810"
      },
      "source": [
        "%run /content/src/train.py --test_file test.txt --test_label_file test_labels.txt --train_batch_size=32 --accum_steps=1 --gpus=4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(accum_steps=1, category_vocab_size=100, dataset_dir='datasets/agnews/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=4, label_names_file='label_names.txt', match_threshold=20, max_len=512, mcp_epochs=3, out_file='out.txt', self_train_epochs=1, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='train.txt', update_interval=50)\n",
            "Effective training batch size: 128\n",
            "Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading encoded texts from datasets/agnews/train.pt\n",
            "Loading texts with label names from datasets/agnews/label_name_data.pt\n",
            "Loading encoded texts from datasets/agnews/test.pt\n",
            "Contructing category vocabulary.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/src/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/src/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOTClassTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Construct category vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Training with masked category prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcp_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/src/trainer.py\u001b[0m in \u001b[0;36mcategory_vocabulary\u001b[0;34m(self, top_pred_num, category_vocab_size, loader_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_vocabulary_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pred_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mgather_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    198\u001b[0m                ' torch.multiprocessing.start_process(...)' % start_method)\n\u001b[1;32m    199\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n    fn(i, *args)\n  File \"/content/src/trainer.py\", line 259, in category_vocabulary_dist\n    model = self.set_up_dist(rank)\n  File \"/content/src/trainer.py\", line 69, in set_up_dist\n    model = self.model.to(rank)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 443, in to\n    return self._apply(convert)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 203, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 203, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 203, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 225, in _apply\n    param_applied = fn(param)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 441, in convert\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\nRuntimeError: CUDA error: invalid device ordinal\n"
          ]
        }
      ]
    }
  ]
}